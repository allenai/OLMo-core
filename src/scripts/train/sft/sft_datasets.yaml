datasets:
  OpenThoughts3-1.2M-no-cot:
    base_dir: tylerr/data/sft/jacobmorrison-OpenThoughts3-1.2M-no-cot
    # NOTE: be careful that this tokenizer matches the pretraining data you've prepared!
    tokenizer: allenai/OLMo-2-1124-7B-Instruct
    token_ids:
      - token_ids_part_0000.npy
      - token_ids_part_0001.npy
    labels:
      - labels_part_0000.npy
      - labels_part_0001.npy
  OpenThoughts3-1.2M:
    base_dir: tylerr/data/sft/jacobmorrison-OpenThoughts3-1.2M
    tokenizer: allenai/OLMo-2-1124-7B-Instruct
    token_ids:
      - token_ids_part_0000.npy
      - token_ids_part_0001.npy
    labels:
      - labels_part_0000.npy
      - labels_part_0001.npy
  OpenThoughts3-456k-gpt4.1-cot:
    base_dir: jacobm/data/sft/jacobmorrison/OpenThoughts3-456k-gpt4.1-cot
    tokenizer: /weka/oe-training-default/ai2-llm/checkpoints/dustins/lc_7b_cont_pretrain_final_anneal/step11921-hf
    token_ids:
      - token_ids_part_0000.npy
      - token_ids_part_0001.npy
      - token_ids_part_0002.npy
      - token_ids_part_0003.npy
      - token_ids_part_0004.npy
    labels:
      - labels_part_0000.npy
      - labels_part_0001.npy
      - labels_part_0002.npy
      - labels_part_0003.npy
      - labels_part_0004.npy
  tulu-3-sft-olmo-2:
    base_dir: tylerr/data/sft/jacobmorrison/tulu-3-sft-olmo-2-mixture
    tokenizer: /weka/oe-training-default/ai2-llm/checkpoints/dustins/lc_7b_cont_pretrain_final_anneal/step11921-hf
    token_ids:
      - token_ids_part_0000.npy
      - token_ids_part_0001.npy
      - token_ids_part_0002.npy
    labels:
      - labels_mask_part_0000.npy 
      - labels_mask_part_0001.npy
      - labels_mask_part_0002.npy    
