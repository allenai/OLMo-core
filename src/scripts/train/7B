2025-05-20 16:31:20.592	Dustin-Schwenk-MacBook-Pro:0	olmo_core.utils:200	INFO	Setting env var 'OMP_NUM_THREADS' to '8'
2025-05-20 16:31:20.592	Dustin-Schwenk-MacBook-Pro:0	olmo_core.utils:200	INFO	Setting env var 'TOKENIZERS_PARALLELISM' to 'false'
ExperimentConfig(
    run_name='run01',
    launch=BeakerLaunchConfig(
        name='run01-train-91f42947',
        cmd=['OLMo2-7B.py', 'train', 'run01', 'ai2/pluto-cirrascale', '--launch.num_nodes=2'],
        budget='ai2/oe-training',
        task_name='train',
        workspace='ai2/OLMo-core',
        description=None,
        setup_steps=[
            'git clone "$REPO_URL" .',
            'git checkout "$GIT_REF"',
            'git submodule update --init --recursive',
            'conda shell.bash activate base',
            "pip install -e '.[all]'",
            'pip install --upgrade beaker-py==1.36.2',
            'pip freeze',
            'mkdir -p ~/.aws',
            'printenv AWS_CONFIG > ~/.aws/config',
            'printenv AWS_CREDENTIALS > ~/.aws/credentials'
        ],
        beaker_image='olmo-core-tch270cu128-2025-05-16',
        num_nodes=2,
        num_gpus=8,
        shared_memory='10GiB',
        clusters=['ai2/pluto-cirrascale'],
        shared_filesystem=True,
        priority=<Priority.normal: 'normal'>,
        preemptible=True,
        retries=None,
        env_vars=[BeakerEnvVar(name='NCCL_DEBUG', value='WARN')],
        env_secrets=[
            BeakerEnvSecret(name='BEAKER_TOKEN', secret='dustins_BEAKER_TOKEN'),
            BeakerEnvSecret(name='WANDB_API_KEY', secret='dustins_WANDB_API_KEY'),
            BeakerEnvSecret(name='AWS_CONFIG', secret='dustins_AWS_CONFIG'),
            BeakerEnvSecret(name='AWS_CREDENTIALS', secret='dustins_AWS_CREDENTIALS'),
            BeakerEnvSecret(name='WEKA_ENDPOINT_URL', secret='WEKA_ENDPOINT_URL')
        ],
        nfs=False,
        weka_buckets=[BeakerWekaBucket(bucket='oe-training-default', mount='/weka/oe-training-default')],
        allow_dirty=False,
        host_networking=None,
        git=None,
        result_dir='/results'
    ),
    model=TransformerConfig(
        d_model=4096,
        vocab_size=100352,
        n_layers=32,
        block=TransformerBlockConfig(
            attention=AttentionConfig(
                name='default',
                n_heads=32,
                n_kv_heads=None,
                bias=False,
                rope=RoPEConfig(name='default', theta=500000, full_precision=True, scaling=None),
                clip_qkv=None,
                qk_norm=LayerNormConfig(name='rms', eps=1e-06, elementwise_affine=None, bias=False, full_precision=None, dtype='float32'),
                dropout=None,
                use_flash=False,
                dtype='float32',
                sliding_window=None
            ),
            layer_norm=LayerNormConfig(name='rms', eps=1e-06, elementwise_affine=None, bias=False, full_precision=None, dtype='float32'),
            feed_forward=FeedForwardConfig(hidden_size=11008, name='default', bias=False, dtype='float32'),
            feed_forward_moe=None,
            name='reordered_norm',
            dropout=None
        ),
        lm_head=LMHeadConfig(
            name='default',
            layer_norm=LayerNormConfig(name='rms', eps=1e-06, elementwise_affine=None, bias=False, full_precision=None, dtype='float32'),
            bias=False,
            dtype='float32',
            loss_implementation='default'
        ),
        name='default',
        dtype='float32',
        init_method='normal',
        init_seed=0,
        init_std=0.02,
        freeze_params=None,
        block_overrides=None
    ),
    dataset=NumpyDatasetConfig(
        tokenizer=TokenizerConfig(vocab_size=100278, eos_token_id=100257, pad_token_id=100277, bos_token_id=None, identifier='allenai/dolma2-tokenizer'),
        name='fsl',
        source_mixture_config=None,
        sequence_length=4096,
        max_target_sequence_length=8192,
        max_sequence_length=None,
        min_sequence_length=None,
        vsl_curriculum=None,
        paths=None,
        mix='OLMoE-mix-0824',
        mix_base_dir='/weka/oe-training-default/ai2-llm',
        dtype=None,
        metadata=None,
        include_instance_metadata=True,
        generate_doc_lengths=False,
        docs_per_instance=None,
        chunks_per_doc=None,
        seed=None,
        interleaving_exempt_paths=None,
        expand_glob=False,
        work_dir='/weka/oe-training-default/ai2-llm/checkpoints/dustins/dataset-cache',
        instance_filter_config=None,
        label_mask_paths=None,
        long_doc_strategy=None
    ),
    data_loader=NumpyDataLoaderConfig(global_batch_size=4194304, seed=34521, work_dir=None, num_threads=None, num_workers=4, prefetch_factor=None, target_device_type=None),
    train_module=TransformerTrainModuleConfig(
        rank_microbatch_size=8192,
        max_sequence_length=4096,
        optim=SkipStepAdamWConfig(
            group_overrides=[OptimGroupOverride(params=['embeddings.weight'], opts={'weight_decay': 0.0})],
            compile=False,
            fixed_fields=['initial_lr'],
            lr=0.0003,
            betas=[0.9, 0.95],
            eps=1e-08,
            weight_decay=0.1,
            rolling_interval_length=128,
            sigma_factor=6,
            dtype=None
        ),
        max_grad_norm=1.0,
        scheduler=CosWithWarmup(lr_field='lr', initial_lr_field='initial_lr', units='steps', warmup=2000, warmup_steps=None, warmup_fraction=None, alpha_f=0.1, t_max=None, warmup_min_lr=0.0),
        compile_model=True,
        float8_config=Float8Config(ao=None, ao_recipe=None, enabled=False),
        pp_config=None,
        dp_config=TransformerDataParallelConfig(name='hsdp', param_dtype='bfloat16', reduce_dtype='float32', num_replicas=None, shard_degree=None, wrapping_strategy='blocks', prefetch_factor=0),
        tp_config=None,
        cp_config=None,
        ep_config=None,
        ac_config=None,
        z_loss_multiplier=1e-05,
        state_dict_save_opts=None,
        state_dict_load_opts=None,
        load_key_mapping=None,
        autocast_precision=None,
        label_ignore_index=-100
    ),
    trainer=TrainerConfig(
        save_folder='/weka/oe-training-default/ai2-llm/checkpoints/dustins/run01',
        work_dir=None,
        load_path=None,
        load_strategy='if_available',
        checkpointer=CheckpointerConfig(work_dir=None, save_overwrite=None, pre_download=False, save_thread_count=None, load_thread_count=None, throttle_uploads=False),
        device=None,
        save_overwrite=True,
        max_duration=Duration(value=4000000000000, unit='tokens'),
        cancel_check_interval=10,
        hard_stop=None,
        metrics_collect_interval=10,
        callbacks={
            'checkpointer': CheckpointerCallback(
                save_interval=10000,
                ephemeral_save_interval=250,
                pre_train_checkpoint=None,
                save_async=True,
                remove='ephemeral_only',
                enabled=True,
                _latest_checkpoint_step=-1,
                _latest_checkpoint_path='',
                _checkpoints=[],
                _ephemeral_checkpoints=[],
                _checkpoints_to_remove=[]
            ),
            'comet': CometCallback(
                enabled=True,
                name='run01',
                project='OLMo-core-7B',
                workspace='ai2',
                tags=None,
                config=None,
                cancel_tags=['cancel', 'canceled', 'cancelled'],
                cancel_check_interval=10,
                notifications='none',
                failure_tag='failed',
                auto_resume=False,
                _exp_key=None,
                _finalized=False
            ),
            'wandb': WandBCallback(
                enabled=False,
                name='run01',
                project='OLMo-core-7B',
                entity='ai2-llm',
                group=None,
                tags=None,
                notes=None,
                config=None,
                cancel_tags=['cancel', 'canceled', 'cancelled'],
                cancel_check_interval=10
            ),
            'downstream_evaluator': DownstreamEvaluatorCallbackConfig(
                tasks=[
                    'arc_challenge_test_mc_5shot_fast',
                    'arc_challenge_test_rc_5shot',
                    'arc_easy_test_mc_5shot_fast',
                    'arc_easy_test_rc_5shot',
                    'basic_skills_arithmetic_rc_5shot',
                    'basic_skills_coding_rc_5shot',
                    'basic_skills_common_knowledge_rc_5shot',
                    'basic_skills_logical_reasoning_rc_5shot',
                    'basic_skills_pattern_rc_5shot',
                    'basic_skills_string_operations_rc_5shot',
                    'codex_humaneval_gold_bpb_3shot',
                    'codex_mbpp_gold_bpb_3shot',
                    'copycolors_10way_fast',
                    'csqa_val_mc_5shot_fast',
                    'csqa_val_rc_5shot',
                    'gsm8k_gold_bpb_5shot',
                    'hellaswag_rc_5shot',
                    'minerva_math_algebra_gold_bpb_0shot',
                    'minerva_math_counting_and_probability_gold_bpb_0shot',
                    'minerva_math_geometry_gold_bpb_0shot',
                    'minerva_math_intermediate_algebra_gold_bpb_0shot',
                    'minerva_math_number_theory_gold_bpb_0shot',
                    'minerva_math_prealgebra_gold_bpb_0shot',
                    'minerva_math_precalculus_gold_bpb_0shot',
                    'mmlu_humanities_test_mc_5shot_fast',
                    'mmlu_humanities_test_rc_5shot',
                    'mmlu_humanities_val_mc_5shot_fast',
                    'mmlu_humanities_val_rc_5shot',
                    'mmlu_other_test_mc_5shot_fast',
                    'mmlu_other_test_rc_5shot',
                    'mmlu_other_val_mc_5shot_fast',
                    'mmlu_other_val_rc_5shot',
                    'mmlu_social_sciences_test_mc_5shot_fast',
                    'mmlu_social_sciences_test_rc_5shot',
                    'mmlu_social_sciences_val_mc_5shot_fast',
                    'mmlu_social_sciences_val_rc_5shot',
                    'mmlu_stem_test_mc_5shot_fast',
                    'mmlu_stem_test_rc_5shot',
                    'mmlu_stem_val_mc_5shot_fast',
                    'mmlu_stem_val_rc_5shot',
                    'mt_mbpp_cpp_gold_bpb_3shot',
                    'mt_mbpp_java_gold_bpb_3shot',
                    'mt_mbpp_rust_gold_bpb_3shot',
                    'piqa_val_mc_5shot_fast',
                    'piqa_val_rc_5shot',
                    'socialiqa_val_mc_5shot_fast',
                    'socialiqa_val_rc_5shot',
                    'winogrande_val_rc_5shot'
                ],
                tokenizer=TokenizerConfig(vocab_size=100278, eos_token_id=100257, pad_token_id=100277, bos_token_id=None, identifier='allenai/dolma2-tokenizer'),
                eval_interval=1000,
                eval_duration=Duration(value=1, unit='epochs'),
                eval_on_startup=False,
                cancel_after_first_eval=False,
                log_interval=5,
                enabled=True
            ),
            'lm_evaluator': LMEvaluatorCallbackConfig(
                eval_dataset=NumpyDatasetConfig(
                    tokenizer=TokenizerConfig(vocab_size=100278, eos_token_id=100257, pad_token_id=100277, bos_token_id=None, identifier='allenai/dolma2-tokenizer'),
                    name='padded_fsl',
                    source_mixture_config=None,
                    sequence_length=4096,
                    max_target_sequence_length=None,
                    max_sequence_length=None,
                    min_sequence_length=None,
                    vsl_curriculum=None,
                    paths=None,
                    mix='v3-small-ppl-validation',
                    mix_base_dir='/weka/oe-training-default/ai2-llm',
                    dtype=None,
                    metadata=None,
                    include_instance_metadata=True,
                    generate_doc_lengths=False,
                    docs_per_instance=None,
                    chunks_per_doc=None,
                    seed=None,
                    interleaving_exempt_paths=None,
                    expand_glob=False,
                    work_dir='/weka/oe-training-default/ai2-llm/checkpoints/dustins/dataset-cache',
                    instance_filter_config=None,
                    label_mask_paths=None,
                    long_doc_strategy=None
                ),
                eval_interval=1000,
                eval_on_startup=False,
                cancel_after_first_eval=False,
                eval_duration=Duration(value=1, unit='epochs'),
                log_interval=5,
                enabled=True
            ),
            'config_saver': ConfigSaverCallback(fname='config.json', save_data_paths=None, data_paths_fname=None, _config=None),
            'profiler': ProfilerCallback(skip_first=0, wait=1, warmup=5, active=3, repeat=1, enabled=False, _first_batch=True),
            'garbage_collector': GarbageCollectorCallback(gc_interval=1000, enabled=True, _start_state=None),
            'slack_notifier': SlackNotifierCallback(name='run01', notifications='end_only', enabled=False, webhook_url=None),
            'beaker': BeakerCallback(experiment_id=None, update_interval=None, description=None, enabled=None, config=None, result_dir='/results', _last_update=None)
        },
        async_bookkeeping=None,
        no_checkpoints=False,
        no_evals=False
    ),
    init_seed=12536
)

Total parameters:         7,298,617,344 (7,298,617,344 active)
Non-embedding parameters: 6,887,575,552 (6,887,575,552 active)
